{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA on Tweets Based On USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T02:41:46.202540Z",
     "start_time": "2020-12-20T02:41:32.580958Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "reviews_datasets = pd.read_csv(r'C:\\Debjit\\ISB\\03_Term-2\\Foundational Project\\Data Dump\\CleanTweets\\FP1_Merged_US_Sentiments.csv')\n",
    "reviews_datasets = reviews_datasets.head(20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T02:41:46.220489Z",
     "start_time": "2020-12-20T02:41:46.207524Z"
    }
   },
   "outputs": [],
   "source": [
    "###cleaning tweets via function\n",
    "import re\n",
    "def clean_tweet(clean_txt):\n",
    "    text = re.sub(\"RT @[\\w]*:\",\"\",clean_txt)\n",
    "    text = re.sub(\"@[\\w]*\",\"\",clean_txt)\n",
    "    text = re.sub(\"https?://[A-Za-z0-9./]*\",\"\",clean_txt)\n",
    "    text = re.sub(\"\\n\",\"\",clean_txt)\n",
    "    return clean_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T02:41:47.577349Z",
     "start_time": "2020-12-20T02:41:46.231113Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>truncated</th>\n",
       "      <th>url</th>\n",
       "      <th>location_US_State</th>\n",
       "      <th>geo_US_State</th>\n",
       "      <th>place_US_State</th>\n",
       "      <th>createDate</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>TB_polarity</th>\n",
       "      <th>TB_subjectivity</th>\n",
       "      <th>AF_polarity</th>\n",
       "      <th>TB_sentiment</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1217234682309464064</td>\n",
       "      <td>2020-01-14T23:58:58</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>A PROVEN LIAR Trump said HE D PAY LEGAL FEES ...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>A PROVEN LIAR Trump said HE D PAY LEGAL FEES ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1217197393948692482</td>\n",
       "      <td>2020-01-14T21:30:47</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Florida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>2 things are often true there is enough evide...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2 things are often true there is enough evide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1217197457270104066</td>\n",
       "      <td>2020-01-14T21:31:02</td>\n",
       "      <td>True</td>\n",
       "      <td>[https://t.co/5R55ma3Efz]</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>Whether you are a Democrat or Republican Trump...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Whether you are a Democrat or Republican Trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1217197459857866753</td>\n",
       "      <td>2020-01-14T21:31:03</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>pubs You don t say Trump Adviser Caught on Ta...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>pubs You don t say Trump Adviser Caught on Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1217197467441213441</td>\n",
       "      <td>2020-01-14T21:31:05</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>So the shooting in Pensacola WAS a terrorist ...</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>Strongly Negative</td>\n",
       "      <td>So the shooting in Pensacola WAS a terrorist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>1323233360060317698</td>\n",
       "      <td>2020-11-02T11:59:52</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>The only economic agrument the haters have fou...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The only economic agrument the haters have fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>1323233426070216705</td>\n",
       "      <td>2020-11-02T12:00:07</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>Im proud to be a hispanic American i studied ...</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.435417</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Im proud to be a hispanic American i studied ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>1323233440980856834</td>\n",
       "      <td>2020-11-02T12:00:11</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>Prediction after loses next week he will quit...</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Prediction after loses next week he will quit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>1323233119793827842</td>\n",
       "      <td>2020-11-02T11:58:54</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>They gotta shut down his Twitter Trump</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>They gotta shut down his Twitter Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>1323234083107987465</td>\n",
       "      <td>2020-11-02T12:02:44</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>I just saw a television advertisement by Trum...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>I just saw a television advertisement by Trum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id           created_at  truncated  \\\n",
       "0      1217234682309464064  2020-01-14T23:58:58      False   \n",
       "1      1217197393948692482  2020-01-14T21:30:47      False   \n",
       "2      1217197457270104066  2020-01-14T21:31:02       True   \n",
       "3      1217197459857866753  2020-01-14T21:31:03      False   \n",
       "4      1217197467441213441  2020-01-14T21:31:05      False   \n",
       "...                    ...                  ...        ...   \n",
       "19995  1323233360060317698  2020-11-02T11:59:52      False   \n",
       "19996  1323233426070216705  2020-11-02T12:00:07      False   \n",
       "19997  1323233440980856834  2020-11-02T12:00:11      False   \n",
       "19998  1323233119793827842  2020-11-02T11:58:54      False   \n",
       "19999  1323234083107987465  2020-11-02T12:02:44      False   \n",
       "\n",
       "                             url         location_US_State geo_US_State  \\\n",
       "0                             []             New Hampshire          NaN   \n",
       "1                             []                   Florida          NaN   \n",
       "2      [https://t.co/5R55ma3Efz]                New Jersey          NaN   \n",
       "3                             []                  Virginia          NaN   \n",
       "4                             []                     Texas          NaN   \n",
       "...                          ...                       ...          ...   \n",
       "19995                         []                  Delaware          NaN   \n",
       "19996                         []             Massachusetts          NaN   \n",
       "19997                         []  United States of America          NaN   \n",
       "19998                         []                   Georgia          NaN   \n",
       "19999                         []                  Maryland          NaN   \n",
       "\n",
       "      place_US_State  createDate  \\\n",
       "0                NaN  2020-01-14   \n",
       "1                NaN  2020-01-14   \n",
       "2                NaN  2020-01-14   \n",
       "3                NaN  2020-01-14   \n",
       "4                NaN  2020-01-14   \n",
       "...              ...         ...   \n",
       "19995            NaN  2020-11-02   \n",
       "19996            NaN  2020-11-02   \n",
       "19997            NaN  2020-11-02   \n",
       "19998            NaN  2020-11-02   \n",
       "19999            NaN  2020-11-02   \n",
       "\n",
       "                                               clean_txt  TB_polarity  \\\n",
       "0       A PROVEN LIAR Trump said HE D PAY LEGAL FEES ...     0.200000   \n",
       "1       2 things are often true there is enough evide...     0.450000   \n",
       "2      Whether you are a Democrat or Republican Trump...     0.000000   \n",
       "3       pubs You don t say Trump Adviser Caught on Ta...     0.000000   \n",
       "4       So the shooting in Pensacola WAS a terrorist ...    -0.800000   \n",
       "...                                                  ...          ...   \n",
       "19995  The only economic agrument the haters have fou...     0.050000   \n",
       "19996   Im proud to be a hispanic American i studied ...     0.177083   \n",
       "19997   Prediction after loses next week he will quit...    -0.150000   \n",
       "19998            They gotta shut down his Twitter Trump     -0.155556   \n",
       "19999   I just saw a television advertisement by Trum...     0.000000   \n",
       "\n",
       "       TB_subjectivity  AF_polarity       TB_sentiment  \\\n",
       "0             0.200000         -5.0            Neutral   \n",
       "1             0.716667          5.0           Positive   \n",
       "2             0.000000          1.0            Neutral   \n",
       "3             0.000000         -2.0            Neutral   \n",
       "4             1.000000         -3.0  Strongly Negative   \n",
       "...                ...          ...                ...   \n",
       "19995         0.400000         -3.0            Neutral   \n",
       "19996         0.435417          1.0            Neutral   \n",
       "19997         0.050000          3.0            Neutral   \n",
       "19998         0.288889          0.0            Neutral   \n",
       "19999         0.000000          0.0            Neutral   \n",
       "\n",
       "                                             clean_tweet  \n",
       "0       A PROVEN LIAR Trump said HE D PAY LEGAL FEES ...  \n",
       "1       2 things are often true there is enough evide...  \n",
       "2      Whether you are a Democrat or Republican Trump...  \n",
       "3       pubs You don t say Trump Adviser Caught on Ta...  \n",
       "4       So the shooting in Pensacola WAS a terrorist ...  \n",
       "...                                                  ...  \n",
       "19995  The only economic agrument the haters have fou...  \n",
       "19996   Im proud to be a hispanic American i studied ...  \n",
       "19997   Prediction after loses next week he will quit...  \n",
       "19998            They gotta shut down his Twitter Trump   \n",
       "19999   I just saw a television advertisement by Trum...  \n",
       "\n",
       "[20000 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaned tweets\n",
    "reviews_datasets['clean_txt'] = reviews_datasets['clean_txt'].apply(lambda x: clean_tweet(x))\n",
    "reviews_datasets.head(20)\n",
    "\n",
    "reviews_datasets['clean_tweet'] = reviews_datasets.clean_txt.apply(clean_tweet)\n",
    "reviews_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T02:42:27.730437Z",
     "start_time": "2020-12-20T02:42:08.774949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 134032 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating vocabulary of all the words in tweets data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "vectorizer = CountVectorizer(\n",
    "analyzer='word',       \n",
    "min_df=3,# minimum required occurences of a word \n",
    "stop_words='english',# remove stop words\n",
    "lowercase=True,# convert all words to lowercase\n",
    "token_pattern='[a-zA-Z0-9]{3,}',# num chars > 3\n",
    "max_features=5000,# max number of unique words\n",
    "                            )\n",
    "data_matrix = vectorizer.fit_transform(reviews_datasets.clean_txt)\n",
    "data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T02:44:27.534031Z",
     "start_time": "2020-12-20T02:42:50.015022Z"
    }
   },
   "outputs": [],
   "source": [
    "#LDA to create topics along with the probability distribution for each word in our vocabulary for each topic\n",
    "lda_model = LatentDirichletAllocation(\n",
    "n_components=10, # Number of topics\n",
    "learning_method='online',\n",
    "random_state=20,       \n",
    "n_jobs = -1  # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T02:44:33.034729Z",
     "start_time": "2020-12-20T02:44:27.539455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pyLDAvis) (2.11.2)\n",
      "Requirement already satisfied: wheel>=0.23.0 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pyLDAvis) (0.34.2)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pyLDAvis) (1.0.5)\n",
      "Requirement already satisfied: funcy in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pyLDAvis) (1.15)\n",
      "Requirement already satisfied: future in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: pytest in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pyLDAvis) (6.2.1)\n",
      "Requirement already satisfied: joblib>=0.8.4 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pyLDAvis) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.18.0 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pyLDAvis) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pyLDAvis) (1.18.5)\n",
      "Requirement already satisfied: numexpr in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2020.1)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pytest->pyLDAvis) (0.4.4)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pytest->pyLDAvis) (1.10.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pytest->pyLDAvis) (20.3.0)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pytest->pyLDAvis) (0.13.1)\n",
      "Requirement already satisfied: toml in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pytest->pyLDAvis) (0.10.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pytest->pyLDAvis) (20.8)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pytest->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0; sys_platform == \"win32\" in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from pytest->pyLDAvis) (1.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.17.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\rayde\\anaconda\\envs\\webscrap\\lib\\site-packages (from packaging->pytest->pyLDAvis) (2.4.7)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2b6206aaf1ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#creating web based visualization for the topic from LDA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install pyLDAvis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "#creating web based visualization for the topic from LDA\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda_model, data_matrix, vectorizer, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T02:39:18.738390Z",
     "start_time": "2020-12-20T02:33:23.700Z"
    }
   },
   "outputs": [],
   "source": [
    "#top 10 words in each topic\n",
    "for i,topic in enumerate(lda_model.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T02:39:18.742608Z",
     "start_time": "2020-12-20T02:33:29.620Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#assigning topic to each Tweet\n",
    "topic_values = lda_model.transform(data_matrix)\n",
    "reviews_datasets['Topic'] = topic_values.argmax(axis=1)\n",
    "reviews_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
