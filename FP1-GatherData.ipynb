{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T12:07:24.000220Z",
     "start_time": "2020-12-20T12:07:23.850857Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tweepy  \n",
    "except ImportError:\n",
    "    %pip install tweepy\n",
    "    import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T12:07:24.605355Z",
     "start_time": "2020-12-20T12:07:24.001150Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import jsonpickle\n",
    "except ImportError:\n",
    "    %pip install jsonpickle\n",
    "    import jsonpickle\n",
    "\n",
    "try:\n",
    "    from newsapi import NewsApiClient\n",
    "except ImportError:\n",
    "    %pip install NewsApiClient\n",
    "    from newsapi import NewsApiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T12:07:25.249197Z",
     "start_time": "2020-12-20T12:07:24.607608Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T12:07:25.259175Z",
     "start_time": "2020-12-20T12:07:25.250193Z"
    }
   },
   "outputs": [],
   "source": [
    "class tweets():\n",
    "    # This function will identify the latest tweet extract file created in the folder and then identify the id\n",
    "    # of the last tweet.\n",
    "    def find_lastTweet(self):\n",
    "        path = os.getcwd()\n",
    "        folder = \"Data Dump/**/FP1_Tweets*.txt\"\n",
    "        fullpath = os.path.join(path, folder)\n",
    "        list_of_files = glob.glob(fullpath, recursive = True)  \n",
    "        if not list_of_files:                \n",
    "            self.lastTweet = None                      \n",
    "        else:\n",
    "            latest_file = max(list_of_files, key=os.path.getctime)\n",
    "            f = open(latest_file,\"r\")\n",
    "            #Read the first tweet\n",
    "            self.lastTweet = int (eachline.split(\",\")[1].split(\":\")[1].strip()) - 1 if eachline else None\n",
    "            f.close()\n",
    "    \n",
    "    def fetch_tweets(self,\n",
    "                     keyword,\n",
    "                     ckey, \n",
    "                     csecret, \n",
    "                     atoken, \n",
    "                     asecret, \n",
    "                     currentTime,\n",
    "                     currDate,\n",
    "                     n_tweets):\n",
    "\n",
    "        print('\\nINFO!! Extracting tweets for ' + keyword)\n",
    "        print('----- Establishing connection')\n",
    "        auth = tweepy.OAuthHandler(ckey, csecret)\n",
    "        auth.set_access_token(atoken, asecret)\n",
    "        api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "        extractedCnt = 0\n",
    "        filePart = 1\n",
    "       \n",
    "        if not (os.path.isdir(\"Data Dump/\"+ currDate)):\n",
    "            os.mkdir(\"Data Dump/\"+ currDate)\n",
    "        # Open the file\n",
    "        f = open(\"Data Dump/\" + currDate + \"/FP1_tweets_\"+currentTime+\"_\"+str(filePart)+\".txt\", \"a\")\n",
    "        for tweet in tweepy.Cursor(api.search,q=\"#Trump\",count=n_tweets,\n",
    "                           lang=\"en\",tweet_mode='extended',result_type='recent',since_id = '1340628236972134400').items():\n",
    "\n",
    "            extractedCnt+=1\n",
    "            #if (extractedCnt > n_tweets):\n",
    "            #    break\n",
    "            if (extractedCnt % 100 == 0):\n",
    "                print(\"      Tweets extracted so far is {0}\".format(extractedCnt))\n",
    "            f.write(jsonpickle.encode(tweet, unpicklable=False) +'\\n')\n",
    "            if (extractedCnt % 200 == 0):\n",
    "                f.close()\n",
    "                filePart += 1\n",
    "                f = open(\"Data Dump/\" + currDate + \"/FP1_tweets_\"+currentTime+\"_\"+str(filePart)+\".txt\", \"a\")\n",
    "        f.close()\n",
    "        print('INFO!! Finished extracting tweets for ' + keyword + '\\n')\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T12:07:25.401386Z",
     "start_time": "2020-12-20T12:07:25.264157Z"
    }
   },
   "outputs": [],
   "source": [
    "class news_bytes():\n",
    "    def fetch_news(self,\n",
    "                   keyword, \n",
    "                   api_key,\n",
    "                   currentTime,\n",
    "                   currDate):\n",
    "        print('\\nINFO!! Extracting news for ' + keyword)\n",
    "        api = NewsApiClient(api_key=api_key)\n",
    "        temp = api.get_everything(q = keyword)\n",
    "\n",
    "        if not (os.path.isdir(\"Data Dump/\"+ currDate)):\n",
    "            os.mkdir(\"Data Dump/\"+ currDate)\n",
    "        # Open the file\n",
    "        f = open(\"Data Dump/\" + currDate + \"/FP1_news_\"+currentTime+\".txt\", \"w\")\n",
    "        f.write(jsonpickle.encode(temp.get('articles'), unpicklable=False) +'\\n')\n",
    "        f.close()\n",
    "        print('INFO!! Finished extracting news for ' + keyword + '\\n')\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-20T12:07:23.853Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Tweets extracted so far is 9000\n",
      "      Tweets extracted so far is 9100\n",
      "      Tweets extracted so far is 9200\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    keywords = ['trump','USElections']\n",
    "\n",
    "    #Fetch Current time for appending to file name\n",
    "    now = datetime.datetime.now()\n",
    "    currentTime=str(now.year)+\"_\"+str(now.month).zfill(2)+\"_\"+str(now.day).zfill(2)+\"_\"+str(now.hour).zfill(2)+\"_\"+str(now.minute).zfill(2)\n",
    "    currDate = str(now.year) + str(now.month).zfill(2) + str(now.day).zfill(2)\n",
    "    print(currentTime)\n",
    "    \n",
    "    # News API Key\n",
    "    api_key ='2d316a0ecc7e47768d88bc7201652b33'\n",
    "    newsBytesObj = news_bytes()\n",
    "    \n",
    "    tweetObj = tweets()\n",
    "    #Number of tweets to be fetched\n",
    "    n_tweets = 100    \n",
    "    # Twitter secrets\n",
    "    ckey = 'lNay2TfnIw61ptsbUdqcmeX0y'\n",
    "    csecret = '1NiVM6dKY7GrYnI4G06s2BoA1Qli7BZFC1ThlQ5JXB29drhEQA'\n",
    "    atoken = '1284353261563068417-sJ3mBP10v0ZKa6v7KY8uDYRAl92xE7'\n",
    "    asecret = 'nxQgeIyMbr5WffFeYFYS0odIKnfwPVUglpeBrRAK9PkGj'\n",
    "    \n",
    "    \n",
    "    \n",
    "    for keyword in keywords:\n",
    "        print('INFO!! Extracting data for ' + keyword)\n",
    "        newsBytesObj.fetch_news(keyword, api_key, currentTime, currDate)    \n",
    "        tweetObj.fetch_tweets(keyword, ckey, csecret, atoken, asecret, currentTime, currDate, n_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
