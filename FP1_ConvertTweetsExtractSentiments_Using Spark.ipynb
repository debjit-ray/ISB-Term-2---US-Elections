{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Spark Session and setup SQL context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T09:20:33.399564Z",
     "start_time": "2020-12-09T09:20:33.234041Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Spark Session (SparkContext and SparkSQL)\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ConvertTweets\").enableHiveSupport().getOrCreate()\n",
    "sqlContext = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T09:20:41.646942Z",
     "start_time": "2020-12-09T09:20:41.334744Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import us\n",
    "import re\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from unidecode import unidecode\n",
    "from textblob import TextBlob\n",
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the corresponding state based on the state code/ partial state name passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fncFindStateByName(location):\n",
    "    state = \"\"\n",
    "    if location:\n",
    "        try:\n",
    "            if (location.lower().find(\"new \") > -1):\n",
    "                location = location.lower().replace(\"new \", \"new_\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "        matchFound = False\n",
    "\n",
    "        for eachWord in location.strip().split():\n",
    "            state = us.states.lookup(eachWord.replace(\"_\",\" \"))\n",
    "            if state:\n",
    "                return state\n",
    "            for eachWord in location.strip().split(\",\"):\n",
    "                state = us.states.lookup(eachWord)\n",
    "                if state:\n",
    "                    return state\n",
    "        state = \"\"\n",
    "        try:\n",
    "            if (location.lower().index('united states')>=0):\n",
    "                state = \"United States of America\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            if (location.lower().index('america')>=0):\n",
    "                state = \"United States of America\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            if (location.lower().index(' usa')>=0):\n",
    "                state = \"United States of America\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            if (location.lower().index('usa ')>=0):\n",
    "                state = \"United States of America\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            if (location.lower().index(',usa')>=0):\n",
    "                state = \"United States of America\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            if (location.lower()=='usa'):\n",
    "                state = \"United States of America\"\n",
    "        except ValueError:\n",
    "            pass    \n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the US state on the basis of the coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fncFindStateByCoordinates(geo):\n",
    "    geolocator = Nominatim(user_agent =\"FP1_Convert_Tweets\")\n",
    "    state = \"\"\n",
    "    if geo:\n",
    "        try:\n",
    "            coordinates = str(geo).split(\"[\")[1].split(\"]\")[0]\n",
    "            location = geolocator.reverse(coordinates)\n",
    "            if str(location).split(',')[-1] ==\" United States\":\n",
    "                state = str(location).split(',')[-3].strip()\n",
    "        except Exception as e:\n",
    "            print (\"Exception: fncFindStateByCoordinates: Processing \" + str(geo) + \". \\n\\tGot exception: \" + str(e))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the US state on the basis of the place provided in the 'place' attribute of tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fncfindStateCode(eachPlace):\n",
    "    if eachPlace:\n",
    "        try:\n",
    "            stateCd = eachPlace.asDict().get('full_name').split(\",\")[1]\n",
    "            if stateCd == 'USA':\n",
    "                stateCd = eachPlace.asDict().get('full_name').split(\",\")[0]\n",
    "            state = fncFindStateByName(stateCd)\n",
    "            return state\n",
    "        except IndexError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print (\"Exception: fncfindStateCode: Processing \" + str(eachPlace) + \". \\n\\tGot exception: \" + str(e))\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T10:05:18.171519Z",
     "start_time": "2020-12-09T10:05:18.165535Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fncProcessFolder(eachFolder):\n",
    "    tweets = None\n",
    "    tweetsF = None\n",
    "    print (\"\\n\\nINFO!!! Processing \" + eachFolder)\n",
    "    fullpath = os.path.join(eachFolder,\"FP1_tweets*.txt\")\n",
    "#   fullpath = os.path.join(eachFolder,\"2020*.jsonl\")\n",
    "    \n",
    "    # Read all the files and load to a temporary table - \"tweets\"\n",
    "    df = sqlContext.read.format(\"json\").options(inferschema = True).load(\"file://\" + fullpath)\n",
    "    # Select only the required fields from the JSON\n",
    "\n",
    "    #    Only for jsonl files\n",
    "    #tweets = df.select(\"id\" ,\"created_at\",\"text\",\"truncated\", \"user.location\",\"geo\", \"place\", \"entities.urls.url\")\n",
    "    #   for files extracted in folders equal or older than 20201123\n",
    "    #tweets = df.select(\"id\" ,\"created_at\",\"text\",\"truncated\", \"_json.user.location\",\"_json.geo\", \"_json.place\", \"_json.entities.urls.url\")\n",
    "    #   for files extracted in folders after 20201123\n",
    "    tweets = df.select(\"id\" ,\"created_at\",\"full_text\",\"truncated\", \"_json.user.location\",\"_json.geo\", \"_json.place\", \"_json.entities.urls.url\")\n",
    "    tweets.registerTempTable(\"tweets\")\n",
    "    tweets.cache()\n",
    "    print (\"\\n\\nINFO!!! Number of tweets read - \" + str(tweets.count()))\n",
    "    \n",
    "    # Clean up the tweet and select only the required columns\n",
    "#    tweets2 = sqlContext.sql(\"select id, created_at, geo, location, place, text,truncated,url from tweets\")\n",
    "    tweets2 = sqlContext.sql(\"select id, created_at, geo, location, place, full_text as text,truncated,url from tweets\")\n",
    "    tweets2.cache()\n",
    "    \n",
    "    # Convert the tweets to Pandas dataframe and find the corresponding State for the location/ geo/ place \n",
    "    # attributes in the tweet\n",
    "    tweetsDF = tweets2.toPandas()\n",
    "\n",
    "    af = Afinn()\n",
    "    # All Lambda functions    \n",
    "    fncRemoveUnicode = lambda x: unidecode(x) if x else \"\"\n",
    "    fncRemoveJunk = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(\\n)\", \" \",str(x)) if x else \"\"\n",
    "    fncCleanTxt = lambda x: re.sub(\"\\s\\s+\",\" \",re.sub(\"(^RT)\",\"\",str(x))) if x else \"\"\n",
    "    fncFindPolarity = lambda x: TextBlob(x).sentiment.polarity\n",
    "    fncFindSubjectivity = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "    fncFindAFScore = lambda x: af.score(x)\n",
    "    fncFindSentiment = lambda x: \"Strongly Negative\" if float(x) <= -0.60 else (\"Negative\" if float(x) <= -0.20 else (\"Neutral\" if float(x) <= 0.20 else (\"Positive\" if float(x) <= 0.60 else \"Strongly Positive\")))\n",
    "\n",
    "    tweetsDF[\"location_US_State\"] = tweetsDF[\"location\"].apply(fncFindStateByName)\n",
    "    tweetsDF[\"geo_US_State\"] = tweetsDF[\"geo\"].apply(fncFindStateByCoordinates)\n",
    "    tweetsDF['place_US_State'] = tweetsDF[\"place\"].apply(fncfindStateCode)\n",
    "    tweetsDF['createDate'] = (pd.to_datetime(tweetsDF[\"created_at\"])).dt.date\n",
    "    tweetsDF['clean_txt'] = ((tweetsDF['text'].apply(fncRemoveUnicode)).apply(fncRemoveJunk)).apply(fncCleanTxt)\n",
    "    tweetsDF.drop(['geo', 'location', 'place','text'], axis = 1, inplace = True)\n",
    "    tweetsDF['TB_polarity'] = tweetsDF['clean_txt'].apply(fncFindPolarity)\n",
    "    tweetsDF['TB_subjectivity'] = tweetsDF['clean_txt'].apply(fncFindSubjectivity)\n",
    "    tweetsDF['AF_polarity'] = tweetsDF['clean_txt'].apply(fncFindAFScore)\n",
    "    tweetsDF['TB_sentiment'] = tweetsDF['TB_polarity'].apply(fncFindSentiment)\n",
    "    tweetsDF = tweetsDF[tweetsDF['clean_txt'] != '']\n",
    "    \n",
    "    # Write the dataframe to csv file if the tweet originated in US\n",
    "    UStweets = tweetsDF[(tweetsDF[\"location_US_State\"] <> \"\") | (tweetsDF[\"place_US_State\"] <> \"\") | (tweetsDF[\"geo_US_State\"] <> \"\")]\n",
    "    UStweets.sort_values(by = 'createDate', ascending = True, inplace = True)\n",
    "    #UStweets.to_csv(\"CleanTweets/FP1_USTweets_\" + eachFolder.split(\"/\")[-2] + \".csv\", encoding = 'utf-8', index = None)\n",
    "    print (\"\\n\\nINFO!!! Count of cleaned US tweets: \" + str(UStweets.count()))\n",
    "    fncWriteToFile(UStweets, \"FP1_USTweets_\" )\n",
    "    nonUStweets = tweetsDF[(tweetsDF[\"location_US_State\"] == \"\") & (tweetsDF[\"place_US_State\"] == \"\") & (tweetsDF[\"geo_US_State\"] == \"\")]\n",
    "    nonUStweets.sort_values(by = 'createDate', ascending = True, inplace = True)\n",
    "    #nonUStweets.to_csv(\"CleanTweets/FP1_nonUSTweets_\" + eachFolder.split(\"/\")[-2] + \".csv\", encoding = 'utf-8', index = None)\n",
    "    print (\"\\n\\nINFO!!! Count of cleaned non US tweets: \" + str(nonUStweets.count()) )\n",
    "    fncWriteToFile(nonUStweets, \"FP1_nonUSTweets_\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fncWriteToFile(tweetFile, pathPrefix):\n",
    "    splitFiles = list(tweetFile.groupby('createDate'))\n",
    "    for eachSplitFile in splitFiles:\n",
    "        if not os.path.exists(\"CleanTweets/\" + str(eachSplitFile[0])):\n",
    "            os.makedirs(\"CleanTweets/\" + str(eachSplitFile[0]))\n",
    "        fileName = \"CleanTweets/\" + str(eachSplitFile[0]) + \"/\" + pathPrefix + str(eachSplitFile[0]) + \".csv\"\n",
    "        if not os.path.isfile(fileName):\n",
    "            eachSplitFile[1].to_csv(fileName, encoding = 'utf-8', index = None)\n",
    "        else:\n",
    "            eachSplitFile[1].to_csv(fileName, encoding = 'utf-8', mode = 'a', header = False, index = None)\n",
    "        print (\"\\n\\nINFO!!! File Name: \" + fileName)\n",
    "        print (\"\\tCount of tweets: \" + str(len(eachSplitFile[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function  - Entry Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "INFO!!! Processing /home/cloudera/Desktop/FP/Data/20201220/\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    path = os.getcwd()\n",
    "    folder = \"Data/*/\"\n",
    "    fullpath = os.path.join(path, folder)\n",
    "    list_of_folders = glob.glob(fullpath)\n",
    "    for eachFolder in list_of_folders:\n",
    "        fncProcessFolder(eachFolder)\n",
    "    print (\"\\n\\nINFO!!! Processing Complete\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda2]",
   "language": "python",
   "name": "Python [anaconda2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
